## Hey there ğŸ‘‹

### About Me
- ğŸ“ Master of Engineering in **Artificial Intelligence** (cum laude) and Bachelor of Science in Computer Science graduate from UniversitÃ  degli studi di Firenze.
- ğŸ“š Iâ€™m currently working on ~~my final thesis about **Federated Learning**~~ LLMs, RAGs and fine-tuning techniques.
- ğŸ§¬ Iâ€™m currently learning **Graph Neural Networks** for path generation.
- ğŸŒ„ **Nature Lover**: Hiking, Biking, and Even the Occasional Cliff Dive. ğŸŒŠ

### Recent Projects

1. ğŸ”¬ **Quantum-Spectral-Clustering-an-hybrid-based-quantum-kernel-approach:** *Exploring Integration of Quantum Techniques with Spectral Clustering*
   - This repository investigates the integration of quantum computing techniques with spectral clustering algorithms, proposing and evaluating a hybrid approach that       combines quantum kernels and feature maps with classical spectral clustering.
   - Extensive experimentation on two datasets, including the ad_hoc dataset from Qiskit and a synthetic complex dataset, reveals that quantum spectral clustering,          particularly utilizing the ZZFeatureMap, outperforms classical methods in identifying clusters, especially in datasets with non-linear separability and high           complexity.
   - However, computational trade-offs exist, with quantum methods showing higher accuracy on smaller datasets but requiring exponentially more computational time          as dataset size increases. 

2. ğŸ›¡ï¸ **Adversarial-Learning-with-FGSM-attacks-and-OOD-Detection:** *Unveiling Vulnerabilities and Securing AI Models* 
   - Repository explores adversarial attacks and OOD detection in PyTorch with three key experiments:
      - 1. OOD detection using "max logits."
      - 2. FGSM attack for robust training.
      - 3. Targeted FGSM attack evaluation, detailed in README. ğŸ¯

3. ğŸ” **Going Deeper with Residual Blocks:** *Why going deeper is not always better, unless you use residual blocks*
   - Repository analyzes neural network depth with MLP and CNNs (VGG16, VGG19, VGG24) for image classification on CIFAR10, also comparing ResNet18 and ResNet34.   
     Explores behavior through gradient and parameter studies. Detailed results in README. ğŸ“ŠğŸ§ ğŸ“ˆ

4. ğŸ¤¹â€â™‚ï¸ **Multi-Task-Attention-Network (MTAN):** *Encoder-Decoder architecture with attention for multi task learning* 
   - Unofficial implementation of 'End-To-End Multi-Task Learning with Attention' (Liu et al., 2019), achieving state-of-the-art 
     results with an Encoder-Decoder architecture and attention modules. ğŸ†

### Keep in touch
- ğŸ“§ Contacts: Write me on [Linkedin](https://www.linkedin.com/in/salah-jebali-dev)
